# ðŸ¤– MCP Local â€“ Servidor de IA com Modelo Offline

## ðŸ“˜ DescriÃ§Ã£o do Projeto
O **MCP Local** Ã© uma aplicaÃ§Ã£o desenvolvida em **Python** com **FastAPI** que executa um **modelo local de linguagem** (como o GPT-2 da Hugging Face) diretamente no servidor, sem depender de conexÃµes externas.  
Seu objetivo Ã© permitir a geraÃ§Ã£o de texto de forma **offline**, rÃ¡pida e segura, mantendo um histÃ³rico completo das interaÃ§Ãµes realizadas pelo usuÃ¡rio.

O projeto Ã© ideal para estudos de **IA local**, **desenvolvimento de chatbots autÃ´nomos**, ou **testes de inferÃªncia com modelos prÃ©-treinados**.

---

## âš™ï¸ Funcionalidades
- ðŸ§  **GeraÃ§Ã£o de texto local** com modelo Hugging Face (ex: GPT-2);  
- ðŸ“´ **ExecuÃ§Ã£o 100% offline**, sem chamadas a APIs externas;  
- ðŸ“œ **HistÃ³rico de interaÃ§Ãµes** via endpoint `/mcp/logs`;  
- ðŸŒ **API RESTful** simples e compatÃ­vel com qualquer frontend;  
- âš¡ **ExecuÃ§Ã£o rÃ¡pida** com o servidor assÃ­ncrono **Uvicorn**.

---

## ðŸ—ºï¸ Diagrama do Fluxo do Sistema

```mermaid
flowchart LR
    A[UsuÃ¡rio] -->|RequisiÃ§Ã£o POST /mcp/local| B[Servidor FastAPI]
    B --> C[Modelo Local de Linguagem]
    C -->|Gera resposta| B
    B -->|Retorna JSON| A
